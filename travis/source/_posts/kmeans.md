---
title: 机器学习算法——kmeans
date: 2017-11-19 04:22:05
tags: nlp, machine-learning, blog
---

## 基本概念
在数据挖掘中，K-Means算法是一种 cluster analysis 的**软聚类**算法，其主要是来计算数据聚集的算法，主要通过不断地取离种子点最近均值的算法。

输入：样本集X，以及k值

输出：对每一个输入，打一个标签y，标示属于某一类

作用：无监督的，将输入X，分成k类

kmeans算法的目标是：针对聚类所得簇划分$C=\{C_1,C_2,...,C_k\}$，最小化均方误差
$$E=\sum_{i=1}^k{\sum_{x \epsilon  C_i}{\left |\left | x- u_i \right |  \right | ^2}}$$

其中$u_i=\frac{1}{C_i}\sum_{x \epsilon  C_i}{x}$是每个簇的聚类中心（簇的均值向量）。直观来看，改式在一定程度描述了簇内样本围绕聚类中心的紧密程度，E值越小说明簇内样本相似度越高

要找到所有可能的划分，解决k-聚类问题，则

* 每个样本点，有1~k种类别选择，复杂度是指数
* 以"每个类的intra-class variance最小化"为评判标准，选择最优的结果
* 即使k=2，这也是一个NP-hard问题

**kmeans算法是一个启发式算法，采用贪心策略，通过迭代优化来近似求解上述问题**。

## 细节部分

**算法流程**

1. 输入样本集X，聚类数k
2. 随机在X中选取k个样本作为中心点（种子点）
3. 对所有点，求其到k个点的距离。如Pi离种子点Si最近，则Pi属于Si点群（类）
4. 至此分成了k个点群（类），对每一个点群，计算新的中心点
5. 重复前两步，直到中心点位置不再变动 or 达到指定的迭代次数

**代码实现**

https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/k_means.py

**复杂度**

1. 假设样本点数为n，features数为m，迭代次数i
2. 计算点距离O(m)——计算某个点属于哪个簇O(mk)——计算所有点属于哪个簇O(nmk)
3. （求平均方法）计算某簇中心点O(n/k)——计算所有点簇中心点O(n)
4. 迭代i次：i*(O(nmk+O(n)) = O(nmki)



kmeans算法是一个启发式算法，复杂度是线性的，找的只是近似解，不是精确解。而且可能是不收敛的（无解的）。



## 问题缺陷
K-means算法的主要缺陷在于初值种子点的选择：

* 受初始种子点影响大。不同的随机种子点会得到完全不同的结果——**kmeans++**改进。
* K要事先给定。但实际上往往并不能事先知道分成多少类合适，所以K的选定是很难以估计的——**ISODATA**改进。



## 改进算法——Kmeans++

K-means++算法在2007被David Arthur提出，用于优化选取k-means算法初值。

**基本思想**

先随机确定一个中心点，在逐个选取出其他中心点，选取的原则是，让聚类中心尽量相互远离。

**算法流程**

1. 确定输入样本集X，聚类数k
2. 随机选取一个样本点作为初始聚类中心
3. 选择新的聚类中心，如何选？
	* 计算所有样本点与当前已有聚类中心的最短距离Di（如计算点A与所有聚类中心距离，保存距离最小的那个）
	* 对每个点i，以$\frac{D_i^2}{\sum_i^N{D_i^2}}$为概率，选取其作为下一个聚类中心。可以用轮盘法，这样距离当前聚类中心越远的点就有更大的概率被选做新的聚类中心
4. 重复2,3步，直到选择出k个聚类中心
5. 选出聚类中心后，计算步骤与kmeans相同

## 改进算法——ISODATA

ISODATA全称为迭代自组织数据分析法。针对kmeans算法中k值难以提前给定，进行了改进。
**基本思想**

当属于某个类别的样本数过少，则去除该类别；当属于某个类别的样本数过多、分散程度过大则将其分裂为两个类别。

**算法流程**

1. 确定输入参数
	* 预期的聚类中心数目K0：虽然聚类中心数可变，但还是要有个参考标准进行
	* 每个类所要求的最少样本数目Nmin：作为**分裂**操作和**合并**操作的判别条件
	* 最大方差Sigma：用于衡量某个类别中的样本分散程度，作为**分裂**操作的判别条件，方差太大的类要分裂
	* 两个聚类中心间的最小距离：作为**合并**的判别条件，距离太近的两个类要合并
2. 随机选取K0个样本作为初始聚类中心
3. 计算每个样本到各聚类中心的距离，将其分到距离最小的类别中
4. 判断每个类中元素是否小于Nmin。若是，则丢弃该类，另K=K-1，并将该类中的样本按照step 3重分类
5. 针对每个聚类，重新计算聚类中心的位置
6. 如果当前总类别数$K<=\frac{K0}{2}$，说明类别太少，进行分裂操作
7. 如果当前总类别数$K>=2*K0$，说明类别太多，进行合并操作
8. 当达到最大迭代次数则终止，否则返回step3继续执行

**整个算法中，迭代操作包括3~7，其中3~5与普通kmeans相同，只是在之后增加了合并操作7与分裂操作6而已**

![merge](merge.png)

![split](split.png)


## 改进算法——kernel kmeans

传统kmeans算法采用欧式距离进行样本空间相似度度量，但对于海量、高维的数据集，这种方式并不一定使用。参照**svm中核函数**的思想，将样本映射到另一个特征空间再进行聚类，有可能改善聚类效果。


经典的k-means采取二次欧氏距离作为相似性的度量，并且假设分类时的聚类误差是服从正态分布的，因此k-means在处理非标准正态分布和非均匀样本集合时聚类效果较差。

对于非标准正态分布和非均匀分布的样本，k-means聚类效果不好，其主要原因是k-means是假设相似度可以用二次欧式距离等价的衡量，但是在实际的样本集合中这个假设不一定实用。

其实关键区别就在于，距离的计算使用核函数（高位空间中样本点的内积）来实现：
$$k(x_i, x_j) = \phi (x_i)^T \phi (x_j)$$

## 改进算法——二分kmeans聚类

按照k-means聚类规则，很容易陷入局部最小值。二分kmeans聚类就是为解决这一问题

**基本思想**

首先把所有样本看成一个簇，而后二分这个簇，接着选择其中一个簇继续进行二分，直到选出了K个簇。至于选取哪个簇继续二分，其选取的原则是二分这个簇后会使总的误差平方和最小。这里的误差平方和可以理解为各样本到聚类中心点的距离和。

**其他的kmeans算法改进还很多，根据具体需要去查询paper**

## 实际应用

* 推荐系统：离线进行聚类，从而在进行协同过滤时仅仅搜索同一个类中的数据，大大降低计算量提升效率
* 市场分类、图像分割、特征学习等

## 参考链接

* **kmeans wiki**：https://en.wikipedia.org/wiki/K-means_clustering
* kmeans++ wiki：https://en.wikipedia.org/wiki/K-means%2B%2B#cite_note-5
* 推荐的中文教程：http://www.csdn.net/article/2012-07-03/2807073-k-means
* 聚类方法tutorial以及demo：http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/AppletKM.html
* 中文blog：http://blog.csdn.net/autocyz/article/details/46754167
* 《机器学习》周志华，西瓜书




