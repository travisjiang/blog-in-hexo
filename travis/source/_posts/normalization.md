---
title: 机器学习——正则化、正规化、规范化、标准化、归一化
date: 2017-11-27 22:21:30
tags:
- machine-learning
categories:
- study
---



## 基本概念

机器学习中常见如下中英文概念，极易混淆：

* 正则化、正规化、标准化、规范化、归一化
* Normalization、Standardization、Regularization

对上述概念可以理解为：

* **Normalization or Standardization（正规化、标准化、规范化、归一化）**是一种数据转化的方式，将数据变化为适合于处理的形式，通常目标目标是对数据进行缩放和和中心化等操作
* **Regularization（正则化）**是求解最优化问题中，用于改善过拟合的手段，常见方式是为Loss函数增加惩罚项

<!--more-->

## Normalization or Standardization


将正规化、规范化、标准化、归一化可以理解为一类任务，就是对数据进行变换，进一步细分的话

* 正规化和规范化，是对数据变化的一种统称方式，包含了归一化和标准化。
* 归一化和标准化，不同评价指标往往具有不同的量纲和单位，为了消除量纲影响，使数据具有可比性。两者可以认为是一回事，也可以认为一些微小区别。



### 归一化

* 目标：将值映射到[0-1]之间
* 方式：比较多，min-max使用较多
	* $x' = \frac{x-min}{max-min}$
	* $x' = \frac{1}{1+e^{-x}}$ or softmax
	* $x' = \log x$
* 好处：最优化问题中，加速梯度迭代收敛速度、提升求解精度


![normal1](normal1.jpg)

### 标准化

* 目标：去中心化，将数据按比例缩放，落入到一个特定的小区间
* 方式：常见zscore和bn
	* zscore(均值为0，方差为1)：$x' = \frac{x-mean}{std}$
	* batch normalization
* 好处：均值为0方差为1有很多优势，如计算距离时使数据重要程度相同等

![normal2](normal2.jpg)

上图是一个散点序列的标准化过程：原图——》去中心化——》除以标准差

### 如何选择做归一化还是标准化？

在涉及到计算点与点之间的距离时，使用归一化或标准化都会对最后的结果有所提升，甚至会有质的区别。那在归一化与标准化之间应该如何选择呢？如果把所有维度的变量一视同仁，在最后计算距离中发挥相同的作用应该选择标准化，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择归一化。另外，标准化更适合现代嘈杂大数据场景。


## Regularization

正则化主要用于避免过拟合的产生和减少网络误差。

正则化一般具有如下形式：

$$J(w, b) = \frac{1}{m}\sum^m_{i=1}{L(f(x),y) + \lambda R(f)}$$

其中，第一项是经验风险项，**第二项就是加入的正则项**。常见的正则项有L1正则和L2正则，对应到范数

* $L_0$范数：$||w||_0 = len(x!=0)$ （非0元素个数）
* $L_1$范数：$||w||_1 = \sum^d_{i=1}{|x_i|}$ （绝对值之和）
* $L_2$范数：$||w||_2 = (\sum^d_{i=1}{x^2_i})^{1/2}$ （欧式距离）
* $L_p$范数：$||w||_p = (\sum^d_{i=1}{x^p_i})^{1/p}$

在机器学习中，若使用了$||w||_p$作为正则项，我们则说该**机器学习任务引入了 $L_{p}$ 正则项**。

L1与L2正则的各自的效果如何？

* L1正则，也叫Lasso regularizer。倾向于得到稀疏解，参数w中只有极少元素非0，有一定的特征选择的效果
* L2正则，也叫Ridge Regularizer/ Weight Decay。倾向于使参数w的大小相对均匀，控制过拟合的效果会比L1稍好。

如何解释上述结论呢？

* 学习的目标是最小化$J(w,b)$，函数由两项组成——经验项和正则项。
* 假设参数是二维点，则划出上述两项的等值线如下图，当总体取到极小值点时，一定是处于两者等值线的交点。
* L1正则项等值线与平方误差经验项的交点较大概率在坐标轴，L2正则项的等值线产生的交点更容易处于非坐标抽区域
* 显然坐标轴上的点，导致其他维度参数为0。而非坐标轴点，则各维度参数较为均匀。

![regular](regular.jpg)




## Reference

* [ML入门：归一化、标准化、正则化](https://zhuanlan.zhihu.com/p/29957294?utm_medium=social&utm_source=weibo)
* [深度学习中Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762)
* 《机器学习》周志华
